#!/usr/bin/env python3
"""Generate a strict Datasheets-for-Datasets style report for the competency catalog.

Outputs:
- PDF datasheet
- Markdown source used for the PDF
- JSON stats/quality-check summary
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import subprocess
import sys
import tempfile
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Generate a strict Datasheets-for-Datasets report from a competency catalog JSON file."
        )
    )
    parser.add_argument(
        "--input",
        default="standardized-comptency-catalog.json",
        help="Path to input JSON file (default: standardized-comptency-catalog.json)",
    )
    parser.add_argument(
        "--output",
        default="standardized-competency-catalog-datasheet.pdf",
        help="Path to output PDF file",
    )
    parser.add_argument(
        "--output-md",
        default="",
        help="Optional explicit Markdown output path (default: <output-stem>.md)",
    )
    parser.add_argument(
        "--output-json",
        default="",
        help="Optional explicit JSON stats output path (default: <output-stem>.stats.json)",
    )
    parser.add_argument(
        "--title",
        default="Datasheet for Dataset: Standardized Competency Catalog",
        help="Datasheet title",
    )
    parser.add_argument(
        "--author",
        default="Auto-generated by scripts/generate_datasheet.py",
        help="Author line shown in the PDF metadata",
    )
    parser.add_argument(
        "--chart-top-n",
        type=int,
        default=12,
        help="Number of knowledge areas to include in the bar chart (default: 12)",
    )
    parser.add_argument(
        "--include-competency-list",
        action="store_true",
        help="Include full competency titles grouped by knowledge area",
    )
    return parser.parse_args()


def read_dataset(path: Path) -> dict[str, Any]:
    if not path.exists():
        raise FileNotFoundError(f"Input file not found: {path}")

    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    if not isinstance(data, dict):
        raise ValueError("Top-level JSON must be an object")

    if "knowledgeAreas" not in data:
        raise ValueError("JSON must contain a 'knowledgeAreas' field")

    if not isinstance(data["knowledgeAreas"], list):
        raise ValueError("'knowledgeAreas' must be an array")

    return data


def normalize_text(value: Any) -> str:
    if value is None:
        return ""
    if isinstance(value, (str, int, float, bool)):
        return str(value).strip()
    return ""


def compute_stats_and_checks(data: dict[str, Any]) -> dict[str, Any]:
    knowledge_areas = data.get("knowledgeAreas", [])
    sources = data.get("sources", []) if isinstance(data.get("sources", []), list) else []

    source_ids = {
        normalize_text(source.get("id"))
        for source in sources
        if isinstance(source, dict) and normalize_text(source.get("id"))
    }

    per_area: list[dict[str, Any]] = []
    taxonomy_counter: Counter[str] = Counter()
    version_counter: Counter[str] = Counter()
    source_counter: Counter[str] = Counter()

    total_competencies = 0
    missing_descriptions = 0

    invalid_ka_type_count = 0
    invalid_competency_type_count = 0

    missing_competency_fields = Counter()
    missing_area_fields = Counter()

    duplicate_ka_titles = Counter()
    ka_title_seen = Counter()

    duplicate_competency_titles_global = Counter()
    global_competency_seen = Counter()

    duplicate_competency_titles_by_area: dict[str, list[str]] = defaultdict(list)

    invalid_source_refs = Counter()
    missing_source_refs = 0

    expected_ka_keys = {"title", "shortTitle", "competencies"}
    expected_comp_keys = {"title", "description", "sourceId", "taxonomy", "version"}

    unexpected_ka_keys = Counter()
    unexpected_comp_keys = Counter()

    for area in knowledge_areas:
        if not isinstance(area, dict):
            invalid_ka_type_count += 1
            continue

        for key in area.keys():
            if key not in expected_ka_keys:
                unexpected_ka_keys[key] += 1

        area_title = normalize_text(area.get("title")) or normalize_text(area.get("shortTitle")) or "Unknown"
        ka_title_seen[area_title] += 1

        for field in ["title", "shortTitle", "competencies"]:
            if field not in area:
                missing_area_fields[field] += 1

        competencies = area.get("competencies")
        if not isinstance(competencies, list):
            competencies = []
            missing_area_fields["competencies(list)"] += 1

        total_competencies += len(competencies)
        local_comp_seen = Counter()

        for comp in competencies:
            if not isinstance(comp, dict):
                invalid_competency_type_count += 1
                continue

            for key in comp.keys():
                if key not in expected_comp_keys:
                    unexpected_comp_keys[key] += 1

            for field in ["title", "description", "sourceId", "taxonomy", "version"]:
                if field not in comp:
                    missing_competency_fields[field] += 1

            ctitle = normalize_text(comp.get("title")) or "Untitled competency"
            local_comp_seen[ctitle] += 1
            global_competency_seen[ctitle] += 1

            taxonomy = normalize_text(comp.get("taxonomy"))
            if taxonomy:
                taxonomy_counter[taxonomy] += 1

            version = normalize_text(comp.get("version"))
            if version:
                version_counter[version] += 1

            source_id = normalize_text(comp.get("sourceId"))
            if source_id:
                source_counter[source_id] += 1
                if source_ids and source_id not in source_ids:
                    invalid_source_refs[source_id] += 1
            else:
                missing_source_refs += 1

            description = normalize_text(comp.get("description"))
            if not description:
                missing_descriptions += 1

        for title, count in local_comp_seen.items():
            if count > 1:
                duplicate_competency_titles_by_area[area_title].append(title)

        per_area.append(
            {
                "knowledge_area": area_title,
                "competency_count": len(competencies),
                "competencies": competencies,
            }
        )

    for ka_title, count in ka_title_seen.items():
        if count > 1:
            duplicate_ka_titles[ka_title] = count

    for ctitle, count in global_competency_seen.items():
        if count > 1:
            duplicate_competency_titles_global[ctitle] = count

    per_area_sorted = sorted(
        per_area,
        key=lambda x: (-x["competency_count"], str(x["knowledge_area"]).lower()),
    )

    avg_per_area = (total_competencies / len(per_area_sorted)) if per_area_sorted else 0.0

    qa_issues = []
    if missing_descriptions > 0:
        qa_issues.append(f"{missing_descriptions} competencies have empty descriptions")
    if missing_source_refs > 0:
        qa_issues.append(f"{missing_source_refs} competencies have missing sourceId")
    if invalid_source_refs:
        qa_issues.append("Some competencies reference sourceId values not found in sources")
    if duplicate_ka_titles:
        qa_issues.append("Duplicate knowledge-area titles detected")
    if duplicate_competency_titles_global:
        qa_issues.append("Duplicate competency titles detected")
    if invalid_ka_type_count > 0 or invalid_competency_type_count > 0:
        qa_issues.append("Invalid JSON item types detected")

    stats = {
        "generated_at": dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "knowledge_area_count": len(per_area_sorted),
        "competency_count": total_competencies,
        "source_count": len(sources),
        "avg_competencies_per_area": avg_per_area,
        "missing_descriptions": missing_descriptions,
        "per_area_sorted": per_area_sorted,
        "taxonomy_counter": dict(taxonomy_counter),
        "version_counter": dict(version_counter),
        "source_counter": dict(source_counter),
        "sources": sources,
        "quality_checks": {
            "invalid_knowledge_area_items": invalid_ka_type_count,
            "invalid_competency_items": invalid_competency_type_count,
            "missing_area_fields": dict(missing_area_fields),
            "missing_competency_fields": dict(missing_competency_fields),
            "duplicate_knowledge_areas": dict(duplicate_ka_titles),
            "duplicate_competency_titles_global": dict(duplicate_competency_titles_global),
            "duplicate_competency_titles_by_area": {
                k: sorted(v) for k, v in duplicate_competency_titles_by_area.items()
            },
            "missing_source_refs": missing_source_refs,
            "invalid_source_refs": dict(invalid_source_refs),
            "unexpected_knowledge_area_keys": dict(unexpected_ka_keys),
            "unexpected_competency_keys": dict(unexpected_comp_keys),
            "issue_summary": qa_issues,
        },
    }
    return stats


def md_escape(value: str) -> str:
    return value.replace("|", "\\|").strip()


def latex_escape(value: str) -> str:
    escaped = value
    replacements = {
        "\\": r"\\textbackslash{}",
        "&": r"\\&",
        "%": r"\\%",
        "$": r"\\$",
        "#": r"\\#",
        "_": r"\\_",
        "{": r"\\{",
        "}": r"\\}",
    }
    for src, target in replacements.items():
        escaped = escaped.replace(src, target)
    return escaped


def short_label(value: str, max_len: int = 24) -> str:
    txt = value.strip()
    if len(txt) <= max_len:
        return txt
    return txt[: max_len - 1].rstrip() + "â€¦"


def chart_label(value: str) -> str:
    # pgfplots symbolic coordinates use commas as separators.
    return short_label(value).replace(",", ";")


def make_bar_chart_latex(stats: dict[str, Any], top_n: int) -> list[str]:
    top_items = stats["per_area_sorted"][: max(1, top_n)]
    if not top_items:
        return ["No data available for chart."]

    ka_ids = [f"KA{i + 1}" for i in range(len(top_items))]
    lines = [
        "```{=latex}",
        "\\begin{figure}[h!]",
        "\\centering",
        "\\begin{tikzpicture}",
        "\\begin{axis}[",
        "xbar,",
        "width=0.82\\textwidth,",
        "height=0.33\\textheight,",
        "xlabel={Number of competencies},",
        "ylabel={Knowledge area ID},",
        "y dir=reverse,",
        "symbolic y coords={",
    ]

    lines.append(",".join(ka_ids) + "},")
    lines.extend(
        [
            "ytick=data,",
            "nodes near coords,",
            "nodes near coords align={horizontal},",
            "tick label style={font=\\scriptsize},",
            "label style={font=\\small},",
            "bar width=8pt,",
            "xmin=0",
            "]",
            "\\addplot coordinates {",
        ]
    )

    for idx, entry in enumerate(top_items):
        area = ka_ids[idx]
        count = int(entry["competency_count"])
        lines.append(f"({count},{area})")

    lines.extend(
        [
            "};",
            "\\end{axis}",
            "\\end{tikzpicture}",
            "\\caption{Competencies per knowledge area (top entries by count). See KA mapping table below.}",
            "\\end{figure}",
            "```",
            "",
        ]
    )

    return lines


def to_markdown(
    stats: dict[str, Any],
    title: str,
    author: str,
    include_competency_list: bool,
    input_path: Path,
    chart_top_n: int,
) -> str:
    qc = stats["quality_checks"]
    lines: list[str] = [
        "---",
        f'title: "{title}"',
        f'author: "{author}"',
        "date: " + dt.date.today().isoformat(),
        "geometry: margin=1in",
        "fontsize: 11pt",
        "colorlinks: true",
        "linkcolor: blue",
        "toc: true",
        "toc-depth: 3",
        "numbersections: true",
        "header-includes:",
        "  - \\usepackage{tikz}",
        "  - \\usepackage{pgfplots}",
        "  - \\pgfplotsset{compat=1.18}",
        "---",
        "",
        "# Section A: High-Level Summary",
        "",
        "## Motivation",
        "This datasheet documents the structure and coverage of the standardized competency catalog for machine-learning and analytics workflows.",
        "",
        "## Composition",
        f"- Input file: {input_path.name}",
        f"- Generated at: `{stats['generated_at']}`",
        f"- Number of knowledge areas: **{stats['knowledge_area_count']}**",
        f"- Number of competencies: **{stats['competency_count']}**",
        f"- Number of sources: **{stats['source_count']}**",
        f"- Average competencies per knowledge area: **{stats['avg_competencies_per_area']:.2f}**",
        f"- Competencies with empty description: **{stats['missing_descriptions']}**",
        "",
        "## Distribution Snapshot (Table)",
        "| Knowledge Area | Competencies |",
        "|---|---:|",
    ]

    for entry in stats["per_area_sorted"][:10]:
        lines.append(f"| {md_escape(str(entry['knowledge_area']))} | {entry['competency_count']} |")

    lines.extend(
        [
            "",
            "## Distribution Snapshot (Chart)",
            "",
        ]
    )
    lines.extend(make_bar_chart_latex(stats, top_n=chart_top_n))
    lines.extend(
        [
            "| ID | Knowledge Area |",
            "|---|---|",
        ]
    )
    for idx, entry in enumerate(stats["per_area_sorted"][: max(1, chart_top_n)]):
        lines.append(f"| KA{idx + 1} | {md_escape(str(entry['knowledge_area']))} |")
    lines.append("")

    lines.extend(
        [
            "## Quality Check Summary",
            "| Check | Result |",
            "|---|---|",
            f"| Missing descriptions | {stats['missing_descriptions']} |",
            f"| Missing sourceId references | {qc['missing_source_refs']} |",
            f"| Invalid sourceId references | {sum(qc['invalid_source_refs'].values())} |",
            f"| Duplicate knowledge area titles | {len(qc['duplicate_knowledge_areas'])} |",
            f"| Duplicate competency titles (global) | {len(qc['duplicate_competency_titles_global'])} |",
            "",
            "# Section B: Detailed Datasheet (Datasheets for Datasets)",
            "",
            "## Collection Process",
            "Collection process details are not explicitly encoded in this JSON dataset and should be documented by dataset maintainers in future revisions.",
            "",
            "## Preprocessing / Cleaning / Labeling",
            "The dataset appears to be pre-structured by knowledge area with competency records containing title, description, taxonomy, sourceId, and version fields.",
            "",
            "## Uses",
            "Potential uses include curriculum mapping, competency gap analysis, and as a structured reference for educational ML/NLP workflows. This datasheet does not validate downstream model suitability.",
            "",
            "## Distribution",
            "This report was generated from a local JSON file and can be redistributed as PDF/Markdown/JSON outputs.",
            "",
            "## Maintenance",
            "Recommended maintenance: version dataset snapshots, monitor schema drift, and re-run this generator in CI for each update.",
            "",
            "## Detailed Composition Tables",
            "### Competencies per knowledge area",
            "| Knowledge Area | Competencies |",
            "|---|---:|",
        ]
    )

    for entry in stats["per_area_sorted"]:
        lines.append(f"| {md_escape(str(entry['knowledge_area']))} | {entry['competency_count']} |")

    lines.extend(
        [
            "",
            "### Taxonomy distribution",
            "| Taxonomy | Count |",
            "|---|---:|",
        ]
    )

    taxonomy_items = sorted(
        stats["taxonomy_counter"].items(), key=lambda item: (-item[1], item[0].lower())
    )
    if taxonomy_items:
        for taxonomy, count in taxonomy_items:
            lines.append(f"| {md_escape(str(taxonomy))} | {count} |")
    else:
        lines.append("| *(none found)* | 0 |")

    lines.extend(
        [
            "",
            "### Version distribution",
            "| Version | Count |",
            "|---|---:|",
        ]
    )

    version_items = sorted(
        stats["version_counter"].items(), key=lambda item: (-item[1], item[0].lower())
    )
    if version_items:
        for version, count in version_items:
            lines.append(f"| {md_escape(str(version))} | {count} |")
    else:
        lines.append("| *(none found)* | 0 |")

    lines.extend(
        [
            "",
            "### Source ID usage",
            "| Source ID | Count |",
            "|---|---:|",
        ]
    )

    source_items = sorted(
        stats["source_counter"].items(), key=lambda item: (-item[1], item[0].lower())
    )
    if source_items:
        for source_id, count in source_items:
            lines.append(f"| {md_escape(str(source_id))} | {count} |")
    else:
        lines.append("| *(none found)* | 0 |")

    lines.extend(
        [
            "",
            "## Intermediate Quality Checks",
            "### Schema sanity",
            "| Check | Value |",
            "|---|---:|",
            f"| Invalid knowledge area item types | {qc['invalid_knowledge_area_items']} |",
            f"| Invalid competency item types | {qc['invalid_competency_items']} |",
            "",
            "### Missing fields",
            "| Field | Missing Count |",
            "|---|---:|",
        ]
    )

    for field, count in sorted(qc["missing_area_fields"].items()):
        lines.append(f"| knowledgeArea.{md_escape(field)} | {count} |")
    for field, count in sorted(qc["missing_competency_fields"].items()):
        lines.append(f"| competency.{md_escape(field)} | {count} |")
    if not qc["missing_area_fields"] and not qc["missing_competency_fields"]:
        lines.append("| *(none found)* | 0 |")

    lines.extend(
        [
            "",
            "### Duplicate detection",
            "| Check | Count |",
            "|---|---:|",
            f"| Duplicate knowledge area titles | {len(qc['duplicate_knowledge_areas'])} |",
            f"| Duplicate competency titles (global) | {len(qc['duplicate_competency_titles_global'])} |",
            "",
            "### Invalid source references",
            "| Source ID | Invalid Reference Count |",
            "|---|---:|",
        ]
    )

    if qc["invalid_source_refs"]:
        for source_id, count in sorted(qc["invalid_source_refs"].items()):
            lines.append(f"| {md_escape(source_id)} | {count} |")
    else:
        lines.append("| *(none found)* | 0 |")

    lines.extend(
        [
            "",
            "## Source Catalog",
            "| Source ID | Title | Author | URI |",
            "|---|---|---|---|",
        ]
    )

    if stats["sources"]:
        for source in stats["sources"]:
            sid = md_escape(str(source.get("id", "")))
            stitle = md_escape(str(source.get("title", "")))
            sauthor = md_escape(str(source.get("author", "")))
            suri = md_escape(str(source.get("uri", "")))
            lines.append(f"| {sid} | {stitle} | {sauthor} | {suri} |")
    else:
        lines.append("| *(none found)* | | | |")

    if include_competency_list:
        lines.extend(["", "## Full Competency Listing"])
        for entry in stats["per_area_sorted"]:
            area_title = str(entry["knowledge_area"])
            lines.append(f"### {area_title}")
            comps = entry["competencies"]
            if not comps:
                lines.append("- No competencies listed.")
                continue
            for comp in comps:
                ctitle = str(comp.get("title", "Untitled competency")).strip()
                tax = str(comp.get("taxonomy", "-")).strip() or "-"
                ver = str(comp.get("version", "-")).strip() or "-"
                lines.append(f"- **{ctitle}** (taxonomy: `{tax}`, version: `{ver}`)")
            lines.append("")

    lines.extend(
        [
            "## Limitations",
            "This report is descriptive. It does not verify semantic correctness of competency descriptions or evaluate model performance impact.",
        ]
    )

    return "\n".join(lines) + "\n"


def write_json_stats(stats: dict[str, Any], output_path: Path) -> None:
    slim_stats = {
        "generated_at": stats["generated_at"],
        "knowledge_area_count": stats["knowledge_area_count"],
        "competency_count": stats["competency_count"],
        "source_count": stats["source_count"],
        "avg_competencies_per_area": stats["avg_competencies_per_area"],
        "missing_descriptions": stats["missing_descriptions"],
        "per_area_counts": [
            {
                "knowledge_area": entry["knowledge_area"],
                "competency_count": entry["competency_count"],
            }
            for entry in stats["per_area_sorted"]
        ],
        "taxonomy_counter": stats["taxonomy_counter"],
        "version_counter": stats["version_counter"],
        "source_counter": stats["source_counter"],
        "quality_checks": stats["quality_checks"],
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(slim_stats, f, indent=2, ensure_ascii=False)


def write_markdown(markdown_text: str, output_path: Path) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(markdown_text, encoding="utf-8")


def run_pandoc(markdown_text: str, output_pdf: Path) -> None:
    output_pdf.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile("w", suffix=".md", encoding="utf-8", delete=False) as tmp:
        tmp.write(markdown_text)
        tmp_path = Path(tmp.name)

    cmd = [
        "pandoc",
        str(tmp_path),
        "-o",
        str(output_pdf),
        "--pdf-engine=xelatex",
        "-V",
        "mainfont=Helvetica",
        "-V",
        "sansfont=Helvetica",
    ]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
    except FileNotFoundError as exc:
        raise RuntimeError(
            "pandoc is not installed or not in PATH. Install pandoc to enable PDF output."
        ) from exc
    except subprocess.CalledProcessError as exc:
        raise RuntimeError(
            "pandoc failed to generate PDF.\n"
            f"Command: {' '.join(cmd)}\n"
            f"stdout: {exc.stdout}\n"
            f"stderr: {exc.stderr}"
        ) from exc
    finally:
        tmp_path.unlink(missing_ok=True)

    if result.stderr.strip():
        print(result.stderr.strip(), file=sys.stderr)


def main() -> int:
    args = parse_args()
    input_path = Path(args.input).resolve()
    output_pdf = Path(args.output).resolve()

    default_md = output_pdf.with_suffix(".md")
    default_json = output_pdf.with_name(output_pdf.stem + ".stats.json")
    output_md = Path(args.output_md).resolve() if args.output_md else default_md
    output_json = Path(args.output_json).resolve() if args.output_json else default_json

    try:
        data = read_dataset(input_path)
        stats = compute_stats_and_checks(data)
        markdown = to_markdown(
            stats=stats,
            title=args.title,
            author=args.author,
            include_competency_list=args.include_competency_list,
            input_path=input_path,
            chart_top_n=args.chart_top_n,
        )

        write_markdown(markdown, output_md)
        write_json_stats(stats, output_json)
        run_pandoc(markdown, output_pdf)

    except Exception as exc:  # pragma: no cover
        print(f"ERROR: {exc}", file=sys.stderr)
        return 1

    print(f"PDF datasheet created: {output_pdf}")
    print(f"Markdown export created: {output_md}")
    print(f"JSON stats export created: {output_json}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
